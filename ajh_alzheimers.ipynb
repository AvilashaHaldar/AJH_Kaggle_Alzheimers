{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8668279,"sourceType":"datasetVersion","datasetId":5194699}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting if Someone Has Alzheimer's\n\nHello! In this notebook, I'll be predicting whether someone has Alzheimer's or not based on medical data. I'll be comparing multiple analysis methods (Naïve Bayes, logistic regression, and maybe a probabilistic neural network if I get time) to see how they perform and explaining why they are or aren't a good idea for this dataset. For each method, I'll try one version with all data columns included, and one version with multicollinear data columns removed, so I can see how performance changes when we remove multicollinearity.\n\nThis notebook will not focus on data exploration, data cleaning, or encoding methods. If you're interested in that, I've covered the basics in [this notebook](https://github.com/AvilashaHaldar/AJH_Kaggle_Housing_Prices_XGBoost).\n\nLet's get started!","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport statistics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, roc_auc_score, roc_curve\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\npd.set_option(\"future.no_silent_downcasting\", True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.864307Z","iopub.execute_input":"2024-07-14T08:24:12.864840Z","iopub.status.idle":"2024-07-14T08:24:12.879130Z","shell.execute_reply.started":"2024-07-14T08:24:12.864803Z","shell.execute_reply":"2024-07-14T08:24:12.877861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_VIF(train_data: pd.DataFrame) -> pd.DataFrame:\n    # VIF dataframe \n    X = train_data.astype(float)\n    vif_data = pd.DataFrame(index=X.columns, columns=[\"VIF\"])\n\n    # calculating VIF for each feature \n    for i in range(len(X.columns)):\n        vif_data.loc[X.columns[i], \"VIF\"] = variance_inflation_factor(X.values, i)\n\n    return vif_data","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.881471Z","iopub.execute_input":"2024-07-14T08:24:12.882563Z","iopub.status.idle":"2024-07-14T08:24:12.890387Z","shell.execute_reply.started":"2024-07-14T08:24:12.882515Z","shell.execute_reply":"2024-07-14T08:24:12.889203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reading in and processing data\n\nFirst, we remove the ID column because it's not useful in predicting Alzheimer's. It's just an index. We also remove the confidential column at the end because every entry there is censored, so again, no useful info. ","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/alzheimers-disease-dataset/alzheimers_disease_data.csv\").drop(columns=[\"PatientID\", \"DoctorInCharge\"])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.905379Z","iopub.execute_input":"2024-07-14T08:24:12.905802Z","iopub.status.idle":"2024-07-14T08:24:12.948682Z","shell.execute_reply.started":"2024-07-14T08:24:12.905768Z","shell.execute_reply":"2024-07-14T08:24:12.947366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The ethnicity column is given in numbers:\n\n- 0: Caucasian\n- 1: African American\n- 2: Asian\n- 3: Other\n\nBut any analysis method we use would assume a meaning to the order of numbers, which isn't true, so we'll one-hot-encode these.\n\nWe'll also turn any int columns that are only 1s and 0s into bool columns. This will make it easier to divide the data later into ones we can use Naïve Bayes on, and ones we can use Gaussian Naïve Bayes on. We base these decisions on the [data description](https://www.kaggle.com/datasets/rabieelkharoua/alzheimers-disease-dataset/data).","metadata":{}},{"cell_type":"code","source":"data[\"Ethnicity\"] = data[\"Ethnicity\"].map(pd.Series({0: \"Caucasian\", 1: \"Black\", 2: \"Asian\", 3: \"Other\"}))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.950885Z","iopub.execute_input":"2024-07-14T08:24:12.951267Z","iopub.status.idle":"2024-07-14T08:24:12.958986Z","shell.execute_reply.started":"2024-07-14T08:24:12.951234Z","shell.execute_reply":"2024-07-14T08:24:12.957720Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ethnicity_columns = pd.get_dummies(data[\"Ethnicity\"])\ndata = pd.concat([data.drop(columns=[\"Ethnicity\"]), ethnicity_columns], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.961073Z","iopub.execute_input":"2024-07-14T08:24:12.961446Z","iopub.status.idle":"2024-07-14T08:24:12.976057Z","shell.execute_reply.started":"2024-07-14T08:24:12.961413Z","shell.execute_reply":"2024-07-14T08:24:12.974858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"int_columns_to_make_bool = [\"Gender\", \"Smoking\", \"FamilyHistoryAlzheimers\", \"CardiovascularDisease\",\n                           \"Diabetes\", \"Depression\", \"HeadInjury\", \"Hypertension\", \"MemoryComplaints\",\n                           \"BehavioralProblems\", \"Confusion\", \"Disorientation\", \"Diagnosis\",\n                           \"PersonalityChanges\", \"DifficultyCompletingTasks\", \"Forgetfulness\"] + ethnicity_columns.columns.tolist()\n\nnon_bool_columns = list(set(data.columns) - set(int_columns_to_make_bool))\n\ndata = pd.concat([data[non_bool_columns], data[int_columns_to_make_bool].astype(bool)], axis=1)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:12.984760Z","iopub.execute_input":"2024-07-14T08:24:12.985222Z","iopub.status.idle":"2024-07-14T08:24:13.019634Z","shell.execute_reply.started":"2024-07-14T08:24:12.985176Z","shell.execute_reply":"2024-07-14T08:24:13.018204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's start looking at dtypes and checking for NaNs.","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.062884Z","iopub.execute_input":"2024-07-14T08:24:13.063294Z","iopub.status.idle":"2024-07-14T08:24:13.075369Z","shell.execute_reply.started":"2024-07-14T08:24:13.063263Z","shell.execute_reply":"2024-07-14T08:24:13.073892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the total number of NaNs in the dataset\ndata.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.077989Z","iopub.execute_input":"2024-07-14T08:24:13.078478Z","iopub.status.idle":"2024-07-14T08:24:13.090438Z","shell.execute_reply.started":"2024-07-14T08:24:13.078432Z","shell.execute_reply":"2024-07-14T08:24:13.088807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oh wow. That's a pleasant surprise - everything is numerical (no object columns) and there aren't any NaNs. That makes the data processing part a lot easier.\n\nI'm thinking of starting with Naïve Bayes (note that we'll use the regular Naïve Bayes for any columns that are only 1s and 0s, as Naïve Bayes checks the probability of a feature existing vs the probability of the label being true; we'll use Gaussian Naïve Bayes for numerical features). My concern with this approach is that all Naïve Bayes methods assume independence of all features, and that is simply not true here.\n\nWe could use the variance inflation factor to find the highly multicollinear features and get rid of them, but honestly, I'm rather curious to see how much worse Naïve Bayes will do with non-independent features than other methods like logistic regression or a neural network. Since this isn't a competition, there's more space for me to try different things.\n\nI'm going to try Naïve Bayes twice - once with the data as is, and once with highly multicollinear columns removed, just to see what happens.\n\n# About Naïve Bayes\n\n**Naïve Bayes** is a popular classification method due to its simplicity and non-black-box behavior. It's relatively easy to understand compared to many deep learning frameworks (e.g. transformers, long short-term memory) and also doesn't take as much computation or time to train. It's not a method that I expect to perform well on this data though, and I'll explain why in a bit.","metadata":{}},{"cell_type":"markdown","source":"## Bayes Theorem\n\nNaïve Bayes is based on **Bayes Theorem**, which is about conditional probability, and tells you the probability of something being true given that something else is already known to be true. The theorem is given below. Note that $P(A|B)$ means probability of $A$ being true if $B$ is true.\n\n$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n\nLet's consider $A$ to be a given label classification, and $B$ to be a feature. If we assume complete independence of all features, we can also consider $B$ to indicate multiple features being true, like so:\n\n$$B = B_0 \\textrm{ and } B_1 \\textrm{ and } ... \\textrm{ and } B_n \\textrm{ all being true}$$\n\n$$P(A|B_0 \\,\\textrm{and}\\, ... B_n\\, \\textrm{being true}) = \\frac{P(\\textrm{all} \\,B \\,\\textrm{features being true} | A) \\times P(A)}{P(\\textrm{all} \\,B \\,\\textrm{features being true})}$$\n\nAssuming independence of features,\n\n$$P(\\textrm{all} \\,B \\,\\textrm{features being true}) = P(B_0) \\times P(B_1) \\times ... \\times P(B_n)$$\n\nNow here's the cool part: we can also set any $B_n$ to be an opposite or a false condition. So we could be looking at $P(\\textrm{(Smoker) and (no Diabetes)})$.\n\n## Naïve Bayes Types\n\nThere are a few different \"types\" of naïve Bayes used for different data types. The classical **Naïve Bayes** is good for boolean features denoting the existence of something, and we then determine how often the existence of that feature correlates to a certain label value. We've got a lot of bool columns (e.g. Diabetes, which is a boolean telling us if a person has diabetes), so we'll use Naïve Bayes for those.\n\n**Gaussian Naïve Bayes** is for numerical features where we determine the mean and standard deviation of the feature assuming a given label value, and then when given a new number, we determine the likelihood of the feature being that value. I'll include some graphs later to make it clearer.\n\nA concern here is that some of our numerical columns aren't actually normally distributed. The data card I linked earlier has frequency plots at the bottom for each feature, and it's quite clear that some numerical features are uniformly distributed. Because of this, I'm expecting logistic regression to work better than Naïve Bayes, but we'll see.\n\nTo start, I'm going to define my own functions for determining Naïve Bayes rather than using the standard sklearn ones. In a production setting, I'd use the sklearn functions, but just as an exercise, I want to try writing the functions myself. We start by defining the columns we want (the boolean ones) and getting the **prior probabilities**, which are the general probability $P(A)$ of a certain label value occurring, where the label values here are True and False diagnoses of Alzheimer's.\n\nWe then get the **conditional probabilities** of a feature being true given a certain label value. We also need the general probabilities of each feature, or the chance of a feature being true regardless of the label value.","metadata":{}},{"cell_type":"code","source":"def implement_full_naive_bayes(data: pd.DataFrame, label_col: str = \"Diagnosis\"):\n    \"\"\"\n    At this point, we assume all columns are either boolean or numerical.\n    We will use Naïve Bayes for the boolean columns (classification)\n    and Gaussian Naïve Bayes for the numerical columns.\n    \"\"\"\n    X = data.drop(columns=[label_col])\n    y = data[label_col]\n    \n    bool_cols = X.dtypes[X.dtypes == bool].index\n    gaussian_cols = X.dtypes[X.dtypes != bool].index\n    \n    unique_classes = y.unique()\n    # The total_probabilities (i.e. the general chance of a certain label value occurring)\n    # are called the prior probabilities.\n    total_probabilities = y.value_counts() / len(y)\n    \n    # General probabilities for each feature, regardless of label value\n    general_probabilities = X[bool_cols].sum() / len(X)\n    general_probabilities = pd.concat([general_probabilities, 1-general_probabilities], axis=1)\n    general_probabilities.columns = [True, False]\n    \n    # We calculate conditional probabilities for each unique possible class/output\n    conditional_probabilities = pd.DataFrame(columns=unique_classes.tolist(), index=bool_cols.tolist())\n    \n    # Here, we get the conditional probabilities\n    for classification in unique_classes:\n        this_df = data[data[label_col] == classification].drop(columns=[label_col])\n        boolean_probabilities = this_df[bool_cols].sum(axis=0) / len(this_df)\n        conditional_probabilities.loc[bool_cols, classification] = boolean_probabilities\n    \n    # conditional_probabilities currently has the columns [True, False] for the label values\n    # and has the probability of each feature column being true given the label value = [conditional_prob column name]\n    # So the True column is the probability that each feature is true given the label is True.\n    \n    # We then create the equivalent dataframe for the probability that each feature is False.\n    conditional_probs_2 = 1-conditional_probabilities\n    \n    # We then make MultiIndex columns with the first level being the label value and the second level being the feature value.\n    conditional_probabilities.columns = pd.MultiIndex.from_tuples(((i, True) for i in conditional_probabilities.columns), names=[\"LabelValue\", \"FeatureValue\"])\n    conditional_probs_2.columns = pd.MultiIndex.from_tuples(((i, False) for i in conditional_probs_2.columns), names=[\"LabelValue\", \"FeatureValue\"])\n\n    # We combine the conditional probability dataframes so we now have every combo of [label_val, feature_val].\n    conditional_probabilities = pd.concat([conditional_probabilities, conditional_probs_2], axis=1)\n    \n    return conditional_probabilities, total_probabilities, general_probabilities, bool_cols, gaussian_cols","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.091984Z","iopub.execute_input":"2024-07-14T08:24:13.092447Z","iopub.status.idle":"2024-07-14T08:24:13.108158Z","shell.execute_reply.started":"2024-07-14T08:24:13.092410Z","shell.execute_reply":"2024-07-14T08:24:13.106809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also need to define how we'll use these values to make new predictions on test_data which does not have the Diagnosis column. \n\nFor classical Naïve Bayes, we will only consider the boolean columns and, for each label value, we get the probability of the feature values being what they are, assuming the label value. This is $P(B|A)$.\n\nWe also have $P(A)$ as the total_probabilities and $P(B)$ as the general_probabilities.\n\nOne thing we need to be careful about is **underflow**, which is when our values get too close to 0 for our computer to remember the digits correctly. Since we'll be multiplying all the probabilities $P(B_0, B_1, ... B_n)$ together, we could end up with very small values, so we'll be **taking the logs of all probabilities**.\n\nWhen Naïve Bayes is used for natural language processing (e.g. classifying an email as spam or not), and the probabilities are of the existence of different words, it's important to deal with the case that test data includes words that didn't exist in the training data. This is often done through **Laplace additive smoothing**, or adding a small number to the probabilities of new words to avoid multiplying by 0 probability, or if we're doing log-transforms, then to avoid taking the log of 0.\n\nIf any of our conditional probabilities were 0, Laplace smoothing would be necessary. However, we don't have that, and I think it's better to not change the probability values themselves if we don't have to, for accuracy's sake.","metadata":{}},{"cell_type":"code","source":"def do_naive_bayes_predictions(conditional_probabilities: pd.DataFrame, total_probabilities: pd.Series, general_probabilities: pd.DataFrame, bool_cols: list, test_data: pd.Series) -> pd.Series:\n    test_data = test_data[bool_cols]\n    naive_bayes_calculations = dict()\n    \n    test_general_probs = np.log(test_data.copy().replace(general_probabilities.T.to_dict()).astype(float)).sum(axis=1)\n    results = pd.DataFrame(index=test_data.index, columns=total_probabilities.index, dtype=float)\n    \n    for labelval in total_probabilities.index:\n        \n        # We get the probability of the features being what they are given that the label = labelval\n        # We also want to take the log of (P(B0) * P(B1) * ... * P(Bn))\n        # which is equivalent to taking the log of each individual probability and then summing the logs\n        \n        this_cond_prob = np.log(test_data.copy().replace(conditional_probabilities[labelval].T.to_dict()).astype(float))\n        \n        # We want to multiply P(B|A) by P(A)/P(B). Since we've taken logs, this is just adding and subtracting\n        this_cond_prob = this_cond_prob.sum(axis=1) + np.log(total_probabilities[labelval]) - test_general_probs\n        results[labelval] = this_cond_prob\n        \n    # At this point, we have the final P(A|B) for each row of test data.\n    # We get the label value corresponding to the highest probability\n    results = results.T.idxmax()\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.111050Z","iopub.execute_input":"2024-07-14T08:24:13.111406Z","iopub.status.idle":"2024-07-14T08:24:13.124618Z","shell.execute_reply.started":"2024-07-14T08:24:13.111374Z","shell.execute_reply":"2024-07-14T08:24:13.123444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_categories(test_predictions: pd.Series, test_answers: pd.Series):\n    all_results = pd.concat([test_predictions, test_answers], axis=1)\n    all_results.columns = [\"predictions\", \"answers\"]\n    \n    true_positives = all_results[(all_results[\"predictions\"] == True) & (all_results[\"answers\"] == True)]\n    true_negatives = all_results[(all_results[\"predictions\"] == False) & (all_results[\"answers\"] == False)]\n    false_positives = all_results[(all_results[\"predictions\"] == True) & (all_results[\"answers\"] == False)]\n    false_negatives = all_results[(all_results[\"predictions\"] == False) & (all_results[\"answers\"] == True)]   \n    \n    return {\"TP\": len(true_positives), \"TN\": len(true_negatives), \"FP\": len(false_positives), \"FN\": len(false_negatives)}\n\ndef make_confusion_matrix(test_predictions: pd.Series, test_answers: pd.Series):\n    all_results = pd.concat([test_predictions, test_answers], axis=1)\n    all_results.columns = [\"predictions\", \"answers\"]\n    \n    cm = confusion_matrix(test_answers, test_predictions, normalize='all')\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n    \n    disp.plot()\n    plt.show()\n    plt.close()\n    \ndef make_ROC_curve(test_predictions: pd.Series, test_answers: pd.Series, label: str=\"\"):\n    \n    auc = roc_auc_score(test_answers, test_predictions)\n    print(f\"Area under ROC curve (AUC): {auc}\")\n    \n    false_positive_rate, true_positive_rate, threshold = roc_curve(test_answers, test_predictions)\n    plt.title('ROC curve')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    \n    plt.plot(false_positive_rate, true_positive_rate, label=label)\n    plt.plot([0, 1], ls=\"--\", label=\"By Chance\")\n    plt.legend()\n    \n    plt.show()\n    plt.close()\n    \n    return auc","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.126318Z","iopub.execute_input":"2024-07-14T08:24:13.126739Z","iopub.status.idle":"2024-07-14T08:24:13.143818Z","shell.execute_reply.started":"2024-07-14T08:24:13.126692Z","shell.execute_reply":"2024-07-14T08:24:13.142348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, so we've looked at how to get the probabilities needed to do Naïve Bayes calculoations with boolean columns. Now let's look at our numerical columns and check if they're Gaussian. First we plot for the case where Diagnosis is True.","metadata":{}},{"cell_type":"code","source":"gaussian_cols = data.dtypes[data.dtypes != bool].index\ndata.loc[data[\"Diagnosis\"] == True][gaussian_cols].hist(figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:13.146179Z","iopub.execute_input":"2024-07-14T08:24:13.146540Z","iopub.status.idle":"2024-07-14T08:24:16.279914Z","shell.execute_reply.started":"2024-07-14T08:24:13.146508Z","shell.execute_reply":"2024-07-14T08:24:16.278609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we plot for where diagnosis is False.","metadata":{}},{"cell_type":"code","source":"data.loc[data[\"Diagnosis\"] == False][gaussian_cols].hist(figsize=(10,10))","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:16.281465Z","iopub.execute_input":"2024-07-14T08:24:16.281860Z","iopub.status.idle":"2024-07-14T08:24:19.370303Z","shell.execute_reply.started":"2024-07-14T08:24:16.281825Z","shell.execute_reply":"2024-07-14T08:24:19.369160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Why is Nothing Normally Distributed?\n\nOh boy. Nearly none of these look normally distributed whatsoever. While classical Naïve Bayes does not assume a normal distribution of features, Gaussian Naïve Bayes _does_ assume a normal distribution of features, which is not true for anything here besides EducationLevel. This is the reason why I don't expect Naïve Bayes to do well on this data.\n\nI suspect that the reason why everything looks oddly uniform is because the data has intentionally cut off the tails. Look at the data description of the clinical measurements:\n\n- SystolicBP: Systolic blood pressure, ranging from 90 to 180 mmHg.\n- DiastolicBP: Diastolic blood pressure, ranging from 60 to 120 mmHg.\n- CholesterolTotal: Total cholesterol levels, ranging from 150 to 300 mg/dL.\n- CholesterolLDL: Low-density lipoprotein cholesterol levels, ranging from 50 to 200 mg/dL.\n- CholesterolHDL: High-density lipoprotein cholesterol levels, ranging from 20 to 100 mg/dL.\n- CholesterolTriglycerides: Triglycerides levels, ranging from 50 to 400 mg/dL.\n\nNow look at the plot from a [research paper](https://www.researchgate.net/figure/Distribution-of-the-low-density-lipoprotein-cholesterol-LDL-C-level-in-34-415-subjects_fig1_231225528) on Cholesterol LDL-C. In the population plot from the paper, the there are values below 50 and above 200 (the cut-offs in the data description) which represent the tails. Same with total cholesterol - [this paper](https://www.researchgate.net/figure/Distribution-of-total-cholesterol-levels-in-study-participants_fig1_51984299) shows total cholesterol values outside the 150-300 range specified in our data. It seems that whoever collected our data either threw out the data from everyone outside the accepted ranges, or simply recorded them as the cut-off values, both of which are wrong.\n\nThis is a PSA to _**ALWAYS**_ record your measurements as they truly are, rather than changing the value if they fall outside the expected range, and to not throw out data just because it looks funny.\n\n## So What Now?\n\nIf the Alzheimer's data hadn't cut off the tails, we could reasonably use Gaussian Naïve Bayes on the clinical data, but since there's no way for us to retrieve the missing tails, we have a few options. In a production setting, I'd move straight to a logistic regression and not even attempt any form of Naïve Bayes, but since this is a space to explore different options to see how well (or how badly) they work, I'm going to try a few things.\n\nFirst, I'll try a version where I disregard the numerical data entirely and only use Naïve Bayes on the boolean columns. I'll then try using Gaussian Bayes on the data as-is, despite it being uniformly distributed. This means trying to fit Gaussians to uniform distribution, and seeing how disastrously that goes.\n\nThen will be trying to transform the uniform distributions to normal ones using various transformations (e.g. log, power, Box-Muller). Let's start with the first version, where we only consider the boolean columns:\n\n# Classical Naïve Bayes Only\n\nThis is an approach I don't like because we're just throwing out tons of features and useful data. But it'll be interesting to see if we can still get semi-sensible results even when we do that. Let's scramble the order of our data and use **K-fold cross-validation** to train and test our data. I've described K-fold cross-validation in more detail previously [here](https://www.kaggle.com/code/avilashahaldar/housing-prices-with-gradient-boosting-trees).","metadata":{}},{"cell_type":"code","source":"def classical_naive_bayes(data, n_splits = 10, label_col = \"Diagnosis\", vif_cutoff=5, remove_multicollinear_cols=False):\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # We train our Naïve Bayes on the training data and test on the testing data, which was not used in training.\n    # We do this for each fold and then average all the errors at the end.\n    kf = KFold(n_splits=n_splits)\n    test_results_dict = dict()\n    all_test_answers = []\n    all_test_predictions = []\n\n    for fold_num, (train_index, test_index) in enumerate(kf.split(data)):\n\n        train_data = data.loc[train_index]\n        test_data = data.loc[test_index].drop(columns=label_col)\n        test_data_answers = data.loc[test_index, label_col]\n        \n        if remove_multicollinear_cols:\n            vif_data = calculate_VIF(train_data.astype(int).astype(float))\n            cols_to_remove = vif_data.loc[vif_data[\"VIF\"] > vif_cutoff].index.tolist()\n            train_data = train_data.drop(columns=cols_to_remove)\n            test_data = test_data.drop(columns=cols_to_remove)\n\n        conditional_probabilities, total_probabilities, general_probabilities, bool_cols, gaussian_cols = implement_full_naive_bayes(train_data)\n        results = do_naive_bayes_predictions(conditional_probabilities, total_probabilities, general_probabilities, bool_cols, test_data)\n\n        test_results_dict[fold_num] = count_categories(results, test_data_answers)\n        all_test_answers.append(test_data_answers)\n        all_test_predictions.append(results)\n\n    test_results_dict = pd.DataFrame(test_results_dict).sum(axis=1)\n    all_test_answers = pd.concat(all_test_answers)\n    all_test_predictions = pd.concat(all_test_predictions)\n    \n    return test_results_dict, all_test_answers, all_test_predictions","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:19.373232Z","iopub.execute_input":"2024-07-14T08:24:19.373686Z","iopub.status.idle":"2024-07-14T08:24:19.386881Z","shell.execute_reply.started":"2024-07-14T08:24:19.373645Z","shell.execute_reply":"2024-07-14T08:24:19.385769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_results_dict, all_test_answers, all_test_predictions = classical_naive_bayes(data, remove_multicollinear_cols=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:19.388216Z","iopub.execute_input":"2024-07-14T08:24:19.388527Z","iopub.status.idle":"2024-07-14T08:24:20.093841Z","shell.execute_reply.started":"2024-07-14T08:24:19.388499Z","shell.execute_reply":"2024-07-14T08:24:20.092101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've added up the true positives, true negatives, etc. from all the folds' test data predictions. Let's now make a confusion matrix and ROC curve.\n\nA **confusion matrix** tells us the proportion of true positives, true negatives, false positives, and false negatives using a 2x2 grid. If your label can be more than 2 possible categories, you can make a confusion matrix for each category, but we have only 2 categories, so just the one confusion matrix is fine.\n\nAn **ROC curve** plots the true positive rate vs the false positive rate. Ideally, we'd have a square; we'd start at the origin and go straight up to (0, 1) and then have a horizontal line across the top, meaning the area under the curve or **AUC** would be 1. If we have a diagonal line from the origin to (1, 1), that means we have an equal number of true and false positives and are basically guessing by chance (assuming there's an equal chance of the label being true or false, which isn't true here, but it's still standard to make this diagonal line anyway).","metadata":{}},{"cell_type":"code","source":"auc = make_ROC_curve(all_test_predictions, all_test_answers, \"Naïve Bayes (boolean cols only)\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:20.095448Z","iopub.execute_input":"2024-07-14T08:24:20.095961Z","iopub.status.idle":"2024-07-14T08:24:20.420179Z","shell.execute_reply.started":"2024-07-14T08:24:20.095905Z","shell.execute_reply":"2024-07-14T08:24:20.418774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_confusion_matrix(all_test_predictions, all_test_answers)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:20.421504Z","iopub.execute_input":"2024-07-14T08:24:20.421851Z","iopub.status.idle":"2024-07-14T08:24:20.717324Z","shell.execute_reply.started":"2024-07-14T08:24:20.421820Z","shell.execute_reply":"2024-07-14T08:24:20.716183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our total proportion of correct predictions is (true positives) + (true negatives) = 0.54 + 0.16 = 0.70, and our AUC is 0.65, so not great. But we expected that. Let's use the **variance inflation factor** to find the multicollinear columns, get rid of the ones that are too multicollinear, and see what happens when we redo classical Naïve Bayes.\n\nSince we want to avoid **data leakage**, we'll be recalculating the VIF for each K-fold and removing columns with a VIF above 5. We don't want to be making decisions about which columns to remove based on data we'll be testing on.\n\nLet's now redo Naïve Bayes while removing the multicollinear columns.","metadata":{}},{"cell_type":"code","source":"test_results_dict, all_test_answers, all_test_predictions = classical_naive_bayes(data, remove_multicollinear_cols=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:20.718722Z","iopub.execute_input":"2024-07-14T08:24:20.719105Z","iopub.status.idle":"2024-07-14T08:24:26.111672Z","shell.execute_reply.started":"2024-07-14T08:24:20.719072Z","shell.execute_reply":"2024-07-14T08:24:26.110115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auc = make_ROC_curve(all_test_predictions, all_test_answers, \"Naïve Bayes (boolean cols only, multicollinear removed)\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:26.120389Z","iopub.execute_input":"2024-07-14T08:24:26.121263Z","iopub.status.idle":"2024-07-14T08:24:26.461943Z","shell.execute_reply.started":"2024-07-14T08:24:26.121224Z","shell.execute_reply":"2024-07-14T08:24:26.460817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"make_confusion_matrix(all_test_predictions, all_test_answers)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:26.463465Z","iopub.execute_input":"2024-07-14T08:24:26.463899Z","iopub.status.idle":"2024-07-14T08:24:26.702289Z","shell.execute_reply.started":"2024-07-14T08:24:26.463859Z","shell.execute_reply":"2024-07-14T08:24:26.701166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Okay, that didn't help at all. I'm not that surprised. With the way Naïve Bayes works, if we have columns that are highly multicollinear, we're almost multiplying by the same probabilities twice, so something previously classified as True is not suddenly going to have the False probability be higher after removing some columns. Let's look at Gaussian Naïve Bayes now.\n\n# Gaussian Naïve Bayes","metadata":{}},{"cell_type":"markdown","source":"## Transforming to Normal Distributions\n\nUniform distributions can be transformed to normal through the **Box-Muller transform**. Normally, this transform takes 2 independent uniform distributions and creates 2 independent normal distributions from them, but I've adapted it to just take 1 uniform distribution and spit out a normal distribution.","metadata":{}},{"cell_type":"code","source":"def box_muller_transform(data_col: pd.Series, to_subtract: float=0.0) -> pd.Series:\n    R = 2*np.log(data_col - to_subtract)\n    theta = 2*np.pi*(data_col - to_subtract)\n    \n    x = R * np.sin(theta)\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:26.703560Z","iopub.execute_input":"2024-07-14T08:24:26.703888Z","iopub.status.idle":"2024-07-14T08:24:26.710249Z","shell.execute_reply.started":"2024-07-14T08:24:26.703858Z","shell.execute_reply":"2024-07-14T08:24:26.709060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = box_muller_transform(data[\"DiastolicBP\"], 59)\nx.hist()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T08:24:26.711801Z","iopub.execute_input":"2024-07-14T08:24:26.712229Z","iopub.status.idle":"2024-07-14T08:24:26.954205Z","shell.execute_reply.started":"2024-07-14T08:24:26.712188Z","shell.execute_reply":"2024-07-14T08:24:26.953212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}